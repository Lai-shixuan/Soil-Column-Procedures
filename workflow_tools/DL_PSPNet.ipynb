{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ALL_PROXY=http://127.0.0.1:33001\n",
      "env: HTTP_PROXY=http://127.0.0.1:33001\n",
      "env: HTTPS_PROXY=http://127.0.0.1:33001\n"
     ]
    }
   ],
   "source": [
    "%env ALL_PROXY=http://127.0.0.1:33001\n",
    "%env HTTP_PROXY=http://127.0.0.1:33001\n",
    "%env HTTPS_PROXY=http://127.0.0.1:33001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD><meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<TITLE>301 Moved</TITLE></HEAD><BODY>\n",
      "<H1>301 Moved</H1>\n",
      "The document has moved\n",
      "<A HREF=\"http://www.google.com/\">here</A>.\n",
      "</BODY></HTML>\n"
     ]
    }
   ],
   "source": [
    "!curl google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If change the model, in training step, you need to \n",
    "1. change the log chapter\n",
    "2. change the model code\n",
    "3. change the wandb chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/root/Soil-Column-Procedures\")\n",
    "\n",
    "from API_functions.DL import load_data, log, seed, evaluate\n",
    "from API_functions import file_batch as fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter and log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parameters = {\n",
    "    'seed': 409,\n",
    "\n",
    "    'Kfold': None,\n",
    "    'ratio': 0.2,\n",
    "\n",
    "    'model': 'PSPNet',   # model = 'U-Net', 'DeepLabv3+', 'PSPNet'\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate':  0.0001,\n",
    "    'batch_size': 32,\n",
    "    'loss_function': 'cross_entropy',\n",
    "\n",
    "    'n_epochs': 1000,\n",
    "    'patience': 50,\n",
    "}\n",
    "\n",
    "device = 'cuda'\n",
    "mylogger = log.Logger('all')\n",
    "\n",
    "seed.stablize_seed(my_parameters['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "transform_train = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=90, p=0.5),\n",
    "    # A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# For validation and test data\n",
    "transform_val = A.Compose([\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.PSPNet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.Unet(\n",
    "#     encoder_name=\"efficientnet-b0\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=my_parameters['learning_rate'])\n",
    "criterion = evaluate.DiceBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes below are only for training.\n",
    "\n",
    "In test step, you need to proceed the codes above and the test chapter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"U-Net\",\n",
    "    name='12.PSPNet with corrected DICE',\n",
    "    config=my_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version0/train_images/', None, 'png')\n",
    "labels_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version0/train_labels/', None, 'png')\n",
    "\n",
    "\n",
    "data = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in data_paths]\n",
    "labels = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in labels_paths]\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=my_parameters['ratio'], random_state=my_parameters['seed'])\n",
    "\n",
    "\n",
    "train_dataset = load_data.my_Dataset(train_data, train_labels, transform=transform_train)\n",
    "val_dataset = load_data.my_Dataset(val_data, val_labels, transform=transform_val)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_parameters['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=my_parameters['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "print(f'len of train_data: {len(train_data)}, len of val_data: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_best = 100000\n",
    "\n",
    "for epoch in range(my_parameters['n_epochs']):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        if outputs.dim() == 4 and outputs.size(1) == 1:\n",
    "            outputs = outputs.squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss_mean = train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            if outputs.dim() == 4 and outputs.size(1) == 1:\n",
    "                outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss_mean = val_loss / len(val_loader.dataset)\n",
    "    dict = {'train_loss': train_loss_mean, 'epoch': epoch, 'val_loss': val_loss_mean}\n",
    "    mylogger.log(dict)\n",
    "\n",
    "    if val_loss_mean < val_loss_best:\n",
    "        val_loss_best = val_loss_mean\n",
    "        torch.save(model.state_dict(), f\"model_{my_parameters['model']}.pth\")\n",
    "        print(f'Model saved at epoch {epoch}, val_loss: {val_loss_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If changes the model\n",
    "1. Test inference6, 7, 8 are for U-Net, DeeplabV3+, PSPNet. Remember to change the path.\n",
    "2. Do change the 'model' chapter in train step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, path):\n",
    "    \"\"\"Save a tensor as an image.\"\"\"\n",
    "    image = image.squeeze().cpu().numpy()\n",
    "    plt.imsave(path, image, cmap='gray')\n",
    "\n",
    "def test_model(model, test_loader, device='cuda'):\n",
    "    # model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Turn off gradients to speed up this part\n",
    "        loss = []\n",
    "        dice = []\n",
    "        soft_dice = []\n",
    "        bce_loss = []\n",
    "        iou = []\n",
    "        f1_score = []\n",
    "\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            if outputs.dim() == 4 and outputs.size(1) == 1:\n",
    "                outputs = outputs.squeeze(1)\n",
    "\n",
    "            # Calculate loss indexes \n",
    "            # 1.bce and dice loss, because the criterion function is used in train process, so it has a sigmoid function inside, should be put before sigmoid\n",
    "            loss.append(criterion(outputs, labels).item())\n",
    "\n",
    "            # 2.Calculate dice, soft_dice, bce_loss \n",
    "            outputs = torch.sigmoid(outputs)  # Apply sigmoid to get values between 0 and 1\n",
    "\n",
    "            dice.append(evaluate.dice_coefficient(outputs, labels))\n",
    "            soft_dice.append(evaluate.soft_dice_coefficient(labels, outputs))\n",
    "            calculate_bce = nn.BCELoss()\n",
    "            bce_loss.append(calculate_bce(outputs, labels))\n",
    "            iou.append(evaluate.iou(pred=outputs, target=labels, n_classes=2))\n",
    "            f1_score.append(evaluate.f1_score(pred=outputs, gt=labels))\n",
    "\n",
    "            # Save output images\n",
    "            outputs = outputs > 0.5  # Threshold the probabilities to create a binary mask\n",
    "            for idx, output in enumerate(outputs):\n",
    "                save_path = f'/root/Soil-Column-Procedures/data/version0/inference/tests_inference8/002_ou_DongYing_{i*test_loader.batch_size + idx + 13635}_roi_selected.png'\n",
    "                save_image(output, save_path)\n",
    "\n",
    "            print(f'Processed batch {i+1}/{len(test_loader)}')\n",
    "        \n",
    "        loss_avg = sum(loss) / len(test_loader)\n",
    "        dice_avg = sum(dice) / len(test_loader)\n",
    "        soft_dice_avg = sum(soft_dice) / len(test_loader)\n",
    "        bce_loss_avg = sum(bce_loss) / len(test_loader)\n",
    "        iou_avg = sum(iou) / len(test_loader)\n",
    "        f1_score_avg = sum(f1_score) / len(test_loader)\n",
    "        print(f'Loss: {loss_avg}, Dice: {dice_avg}, soft_dice: {soft_dice_avg}, BCE Loss: {bce_loss_avg}, IOU: {iou_avg}, f1_score: {f1_score_avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model, using the model define before\n",
    "\n",
    "model.load_state_dict(torch.load(f\"model_{my_parameters['model']}.pth\", weights_only=True))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 images have been found in /root/Soil-Column-Procedures/data/version0/test_images/\n",
      "The first 3 images are:\n",
      "/root/Soil-Column-Procedures/data/version0/test_images/002_ou_DongYing_13635_roi_selected.png\n",
      "/root/Soil-Column-Procedures/data/version0/test_images/002_ou_DongYing_13636_roi_selected.png\n",
      "/root/Soil-Column-Procedures/data/version0/test_images/002_ou_DongYing_13637_roi_selected.png\n",
      "\u001b[1;3mGet names completely!\u001b[0m\n",
      "5 images have been found in /root/Soil-Column-Procedures/data/version0/test_labels/\n",
      "The first 3 images are:\n",
      "/root/Soil-Column-Procedures/data/version0/test_labels/002_ou_DongYing_13635_roi_selected.png\n",
      "/root/Soil-Column-Procedures/data/version0/test_labels/002_ou_DongYing_13636_roi_selected.png\n",
      "/root/Soil-Column-Procedures/data/version0/test_labels/002_ou_DongYing_13637_roi_selected.png\n",
      "\u001b[1;3mGet names completely!\u001b[0m\n",
      "len of test_data: 5\n"
     ]
    }
   ],
   "source": [
    "test_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version0/test_images/', None, 'png')\n",
    "test_labels_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version0/test_labels/', None, 'png')\n",
    "tests = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in test_paths]\n",
    "test_labels = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in test_labels_paths]\n",
    "\n",
    "\n",
    "test_dataset = load_data.my_Dataset(tests, test_labels, transform=transform_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_parameters['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "print(f'len of test_data: {len(tests)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Soil-Column-Procedures/API_functions/DL/load_data.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float32)\n",
      "/root/Soil-Column-Procedures/API_functions/DL/load_data.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label/255, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n",
      "Loss: 0.09574916213750839, Dice: 0.7797616124153137, soft_dice: 0.8170031905174255, BCE Loss: 0.00850151851773262, IOU: 0.639023984702997, f1_score: 0.7797616028404768\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from API_functions.Visual import file_compare as fc\n",
    "# %matplotlib qt\n",
    "\n",
    "db = fc.ImageDatabase()\n",
    "# image_processor.add_result('pre_processed', tpi.user_threshold(image_processor.image, 160))\n",
    "zoom = fc.ZoomRegion(200, 400, 100, 200)\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version0/test_images/', 'test_images')\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version0/test_labels/', 'test_labels')\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version0/inference/tests_inference7/', 'test_inference')\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version0/inference/tests_inference6/', 'test_inference-ori')\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version0/inference/tests_inference8/', 'test_inference-PSP')\n",
    "image_processor = db.get_image_processor('002_ou_DongYing_13636_roi_selected.png')\n",
    "image_processor.show_images('test_images', 'test_inference', 'test_labels', \"test_inference-ori\", \"test_inference-PSP\", zoom_region=zoom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
