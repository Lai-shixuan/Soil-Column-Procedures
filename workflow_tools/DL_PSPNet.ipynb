{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ALL_PROXY=http://127.0.0.1:33001\n",
    "%env HTTP_PROXY=http://127.0.0.1:33001\n",
    "%env HTTPS_PROXY=http://127.0.0.1:33001\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If change the model, in training step, you need to \n",
    "1. change the log chapter\n",
    "2. change the model code\n",
    "3. change the wandb chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/root/Soil-Column-Procedures\")\n",
    "\n",
    "from API_functions.DL import load_data, log, seed, evaluate, precheck\n",
    "from API_functions import file_batch as fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter and log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parameters = {\n",
    "    'seed': 409,\n",
    "\n",
    "    'Kfold': None,\n",
    "    'ratio': 0.2,\n",
    "\n",
    "    'model': 'DeepLabv3+',   # model = 'U-Net', 'DeepLabv3+', 'PSPNet'\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate':  0.0001,\n",
    "    'batch_size': 32,\n",
    "    'loss_function': 'cross_entropy',\n",
    "\n",
    "    'n_epochs': 1000,\n",
    "    'patience': 50,\n",
    "}\n",
    "\n",
    "device = 'cuda'\n",
    "mylogger = log.Logger('all')\n",
    "\n",
    "seed.stablize_seed(my_parameters['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "transform_train = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=90, p=0.5),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    # A.OpticalDistortion(p=0.5),\n",
    "    ToTensorV2(),\n",
    "], seed=my_parameters['seed'])\n",
    "\n",
    "# For validation and test data\n",
    "transform_val = A.Compose([\n",
    "    ToTensorV2(),\n",
    "], seed=my_parameters['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.PSPNet(\n",
    "#     encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.Unet(\n",
    "#     encoder_name=\"efficientnet-b0\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=my_parameters['learning_rate'])\n",
    "criterion = evaluate.DiceBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes below are only for training.\n",
    "\n",
    "In test step, you need to proceed the codes above and the test chapter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"U-Net\",\n",
    "    name='13.DeepLab, add noisy data',\n",
    "    config=my_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version0/train_images/', None, 'png')\n",
    "labels_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version0/train_labels/', None, 'png')\n",
    "\n",
    "\n",
    "data = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in data_paths]\n",
    "labels = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in labels_paths]\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=my_parameters['ratio'], random_state=my_parameters['seed'])\n",
    "\n",
    "\n",
    "train_dataset = load_data.my_Dataset(train_data, train_labels, transform=transform_train)\n",
    "val_dataset = load_data.my_Dataset(val_data, val_labels, transform=transform_val)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_parameters['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=my_parameters['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "print(f'len of train_data: {len(train_data)}, len of val_data: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_best = 100000\n",
    "proceed_once = True  # Add a flag\n",
    "\n",
    "for epoch in range(my_parameters['n_epochs']):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Checking the dimension of the outputs and labels\n",
    "        if outputs.dim() == 4 and outputs.size(1) == 1:\n",
    "            outputs = outputs.squeeze(1)\n",
    "        \n",
    "        # Only proceed once:\n",
    "        if proceed_once:\n",
    "            print(f'outputs.size(): {outputs.size()}, labels.size(): {labels.size()}')\n",
    "            print(f'outputs.min: {outputs.min()}, outputs.max: {outputs.max()}')\n",
    "            print(f'labels.min: {labels.min()}, labels.max: {labels.max()}')\n",
    "            print(f'count of label 0: {(labels == 0).sum()}, count of label 1:{(labels == 1).sum()}')\n",
    "            print('')\n",
    "            proceed_once = False  # Set the flag to False after proceeding once\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss_mean = train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            if outputs.dim() == 4 and outputs.size(1) == 1:\n",
    "                outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss_mean = val_loss / len(val_loader.dataset)\n",
    "    dict = {'train_loss': train_loss_mean, 'epoch': epoch, 'val_loss': val_loss_mean}\n",
    "    mylogger.log(dict)\n",
    "\n",
    "    if val_loss_mean < val_loss_best:\n",
    "        val_loss_best = val_loss_mean\n",
    "        torch.save(model.state_dict(), f\"model_{my_parameters['model']}.pth\")\n",
    "        print(f'Model saved at epoch {epoch}, val_loss: {val_loss_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If changes the model\n",
    "1. Test inference6, 7, 8 are for U-Net, DeeplabV3+, PSPNet. Remember to change the path.\n",
    "2. Do change the 'model' chapter in train step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If changes the data\n",
    "1. Test inference path\n",
    "2. Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, test_names, device='cuda'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Turn off gradients to speed up this part\n",
    "        loss = []\n",
    "        dice = []\n",
    "        soft_dice = []\n",
    "        bce_loss = []\n",
    "        iou = []\n",
    "        f1_score = []\n",
    "\n",
    "        proceed_once = True  # Add a flag\n",
    "\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Checking the dimension of the outputs and labels\n",
    "            if outputs.dim() == 4 and outputs.size(1) == 1:\n",
    "                outputs = outputs.squeeze(1)\n",
    "\n",
    "            # Only proceed once:\n",
    "            if proceed_once:\n",
    "                print(f'outputs.size(): {outputs.size()}, labels.size(): {labels.size()}')\n",
    "                print(f'outputs.min: {outputs.min()}, outputs.max: {outputs.max()}')\n",
    "                print(f'labels.min: {labels.min()}, labels.max: {labels.max()}')\n",
    "                print(f'count of label 0: {(labels == 0).sum()}, count of label 1:{(labels == 1).sum()}')\n",
    "                print('')\n",
    "                proceed_once = False\n",
    "\n",
    "            # Calculate loss indexes \n",
    "            # 1.bce and dice loss, because the criterion function is used in train process, so it has a sigmoid function inside, should be put before sigmoid\n",
    "            loss.append(criterion(outputs, labels).item())\n",
    "\n",
    "            # 2.Calculate dice, soft_dice, bce_loss \n",
    "            outputs = torch.sigmoid(outputs)  # Apply sigmoid to get values between 0 and 1\n",
    "\n",
    "            dice.append(evaluate.dice_coefficient(outputs, labels))\n",
    "            soft_dice.append(evaluate.soft_dice_coefficient(labels, outputs))\n",
    "            calculate_bce = nn.BCELoss()\n",
    "            bce_loss.append(calculate_bce(outputs, labels))\n",
    "            iou.append(evaluate.iou(pred=outputs, target=labels, n_classes=2))\n",
    "            f1_score.append(evaluate.f1_score(pred=outputs, gt=labels))\n",
    "\n",
    "            # Save output images\n",
    "            outputs = outputs > 0.5  # Threshold the probabilities to create a binary mask\n",
    "            for j, img in enumerate(outputs):\n",
    "                save_path = f'/root/Soil-Column-Procedures/data/version1/inference/1/'\n",
    "                save_path = save_path + test_names[j]\n",
    "                output_np = img.cpu().numpy().astype(np.uint8) * 255  # Convert to numpy array and scale to 0-255\n",
    "                cv2.imwrite(save_path, output_np)\n",
    "\n",
    "            print(f'Processed batch {i+1}/{len(test_loader)}')\n",
    "        \n",
    "        loss_avg = sum(loss) / len(test_loader)\n",
    "        dice_avg = sum(dice) / len(test_loader)\n",
    "        soft_dice_avg = sum(soft_dice) / len(test_loader)\n",
    "        bce_loss_avg = sum(bce_loss) / len(test_loader)\n",
    "        iou_avg = sum(iou) / len(test_loader)\n",
    "        f1_score_avg = sum(f1_score) / len(test_loader)\n",
    "        print(f'Loss: {loss_avg}, Dice: {dice_avg}, soft_dice: {soft_dice_avg}, BCE Loss: {bce_loss_avg}, IOU: {iou_avg}, f1_score: {f1_score_avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model, using the model define before\n",
    "\n",
    "model.load_state_dict(torch.load(f\"model_{my_parameters['model']}.pth\", weights_only=True))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 images have been found in /root/Soil-Column-Procedures/data/version1/test_images/\n",
      "All images are:\n",
      "/root/Soil-Column-Procedures/data/version1/test_images/0028.385.png\n",
      "\u001b[1;3mGet names completely!\u001b[0m\n",
      "1 images have been found in /root/Soil-Column-Procedures/data/version1/test_labels/\n",
      "All images are:\n",
      "/root/Soil-Column-Procedures/data/version1/test_labels/0028.385.png\n",
      "\u001b[1;3mGet names completely!\u001b[0m\n",
      "Check 1 pass, Dataset and labels have the same shape\n",
      "2.Dataset images have been padded\n",
      "2.Label images have been padded\n",
      "Check 3 pass, Images and labels are 8-bit\n",
      "Check 4 pass, Labels contain only 0 and 255\n",
      "len of test_data: 1\n"
     ]
    }
   ],
   "source": [
    "test_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version1/test_images/', None, 'png')\n",
    "test_labels_paths = fb.get_image_names('/root/Soil-Column-Procedures/data/version1/test_labels/', None, 'png')\n",
    "tests = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in test_paths]\n",
    "test_labels = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in test_labels_paths]\n",
    "\n",
    "tests, test_labels = precheck.precheck(tests, test_labels)\n",
    "\n",
    "test_dataset = load_data.my_Dataset(tests, test_labels, transform=transform_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_parameters['batch_size'], shuffle=False)\n",
    "\n",
    "print(f'len of test_data: {len(tests)}')\n",
    "\n",
    "test_names = [p.split('/')[-1] for p in test_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.size(): torch.Size([1, 800, 800]), labels.size(): torch.Size([1, 800, 800])\n",
      "outputs.min: -10.441927909851074, outputs.max: 26.676090240478516\n",
      "labels.min: 0.0, labels.max: 1.0\n",
      "count of label 0: 635364, count of label 1:4636\n",
      "\n",
      "Processed batch 1/1\n",
      "Loss: 0.0572110153734684, Dice: 0.8596512675285339, soft_dice: 0.8950426578521729, BCE Loss: 0.0094646867364645, IOU: 0.7538494439692045, f1_score: 0.8596512620412144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Soil-Column-Procedures/API_functions/DL/load_data.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float32)\n",
      "/root/Soil-Column-Procedures/API_functions/DL/load_data.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label/255, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from API_functions.Visual import file_compare as fc\n",
    "# %matplotlib qt\n",
    "\n",
    "db = fc.ImageDatabase()\n",
    "# image_processor.add_result('pre_processed', tpi.user_threshold(image_processor.image, 160))\n",
    "zoom = fc.ZoomRegion(200, 400, 100, 200)\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version1/test_images/', 'test_images')\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version1/test_labels/', 'test_labels')\n",
    "db.add_additional_folder('/root/Soil-Column-Procedures/data/version1/inference/1/', 'test_inference')\n",
    "image_processor = db.get_image_processor('0028.386.png')\n",
    "image_processor.show_images('test_images', 'test_inference', 'test_labels', zoom_region=zoom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
